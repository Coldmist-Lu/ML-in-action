{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization 正则化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 $\\ell_1$ and $\\ell_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for L1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both L1 and L2 regularization, with facters 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "55000/55000 [==============================] - 10s 190us/sample - loss: 1.5670 - accuracy: 0.8114 - val_loss: 0.7278 - val_accuracy: 0.8206\n",
      "Epoch 2/2\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: 0.7178 - accuracy: 0.8276 - val_loss: 0.6953 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='elu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation='elu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation='softmax',\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "55000/55000 [==============================] - 11s 194us/sample - loss: 1.6559 - accuracy: 0.8135 - val_loss: 0.7382 - val_accuracy: 0.8236\n",
      "Epoch 2/2\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: 0.7189 - accuracy: 0.8259 - val_loss: 0.6798 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "# 用 partial 简单包装一下\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Dropout\n",
    "\n",
    "* 暂时删除的概率 p，一般 RNN 中为 20 ~ 30%，CNN 中为 40 ~ 50%。\n",
    "* Dropout 可以看成是每次训练一个独特网络，多个网络的复杂集成。\n",
    "* 通常只可以对 第一层至第三层 使用Dropout。许多最新的架构在最后一层采用 Dropout.\n",
    "* 一个很重要的技术细节：训练后各个神经元的权重需要乘以保留概率（1-p），这是由于训练的时候输入神经元的数量与测试时不同。\n",
    "* 由于 dropout 仅在训练期间激活，模型很可能会过拟合训练集，所以不应该使用 dropout 来评估训练损失。\n",
    "* 使用 SELU 作为激活函数的时候，为了保证 dropout 后均值和方差不变，应该采用 alpha dropout。\n",
    "\n",
    "### 2-1 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "55000/55000 [==============================] - 11s 198us/sample - loss: 0.5671 - accuracy: 0.8016 - val_loss: 0.3942 - val_accuracy: 0.8516\n",
      "Epoch 2/2\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.4216 - accuracy: 0.8449 - val_loss: 0.3701 - val_accuracy: 0.8604\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Alpha Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 9s 155us/sample - loss: 0.6626 - accuracy: 0.7588 - val_loss: 0.6112 - val_accuracy: 0.8414\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 8s 139us/sample - loss: 0.5565 - accuracy: 0.7943 - val_loss: 0.5611 - val_accuracy: 0.8370\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 8s 141us/sample - loss: 0.5253 - accuracy: 0.8052 - val_loss: 0.5581 - val_accuracy: 0.8392\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 8s 142us/sample - loss: 0.5064 - accuracy: 0.8141 - val_loss: 0.4935 - val_accuracy: 0.8562\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 8s 146us/sample - loss: 0.4910 - accuracy: 0.8186 - val_loss: 0.4519 - val_accuracy: 0.8578\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 8s 149us/sample - loss: 0.4783 - accuracy: 0.8223 - val_loss: 0.4771 - val_accuracy: 0.8578\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 8s 147us/sample - loss: 0.4692 - accuracy: 0.8268 - val_loss: 0.4553 - val_accuracy: 0.8620\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 8s 147us/sample - loss: 0.4626 - accuracy: 0.8280 - val_loss: 0.4654 - val_accuracy: 0.8624\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 8s 144us/sample - loss: 0.4592 - accuracy: 0.8299 - val_loss: 0.4341 - val_accuracy: 0.8706\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 8s 143us/sample - loss: 0.4509 - accuracy: 0.8310 - val_loss: 0.4369 - val_accuracy: 0.8624\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 8s 148us/sample - loss: 0.4525 - accuracy: 0.8323 - val_loss: 0.4080 - val_accuracy: 0.8714\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 8s 144us/sample - loss: 0.4438 - accuracy: 0.8348 - val_loss: 0.4429 - val_accuracy: 0.8626\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 8s 146us/sample - loss: 0.4383 - accuracy: 0.8391 - val_loss: 0.4561 - val_accuracy: 0.8696\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 8s 146us/sample - loss: 0.4373 - accuracy: 0.8346 - val_loss: 0.4672 - val_accuracy: 0.8682\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 8s 151us/sample - loss: 0.4298 - accuracy: 0.8407 - val_loss: 0.4434 - val_accuracy: 0.8756\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 8s 151us/sample - loss: 0.4309 - accuracy: 0.8393 - val_loss: 0.4276 - val_accuracy: 0.8776\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 8s 150us/sample - loss: 0.4265 - accuracy: 0.8419 - val_loss: 0.4295 - val_accuracy: 0.8772\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 8s 151us/sample - loss: 0.4193 - accuracy: 0.8425 - val_loss: 0.4414 - val_accuracy: 0.8780\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 8s 147us/sample - loss: 0.4235 - accuracy: 0.8427 - val_loss: 0.4263 - val_accuracy: 0.8688\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 8s 147us/sample - loss: 0.4222 - accuracy: 0.8425 - val_loss: 0.4161 - val_accuracy: 0.8798\n"
     ]
    }
   ],
   "source": [
    "# alpha dropout\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Monte Carlo Dropout (MC Dropout)\n",
    "\n",
    "* 小 trick，即用多次加了 Dropout 的层进行预测平均。\n",
    "* 这样的估计通常比关闭 Dropout 的估计更加可靠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.2 , 0.  , 0.73]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.21, 0.  , 0.24]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果其它层有使用 BatchNormalization 等特殊训练方式，那么应该使用 MCDropout 类来替换 Dropout 层。\n",
    "* 具体方法是：先继承一个MCDropout类，强制定义可训练；然后将所有dropout替换为MCDropout，最后在新的模型上运用旧模型的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "    \n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer # 将所有dropout layer替换成MCDropout\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_3 (MCAlphaD (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_4 (MCAlphaD (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_5 (MCAlphaD (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights()) # 将新的模型的权重用旧的模型权重覆盖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.2 , 0.  , 0.72]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Max Norm 最大范数正则化\n",
    "\n",
    "* 通过限制范数的最大值来减少过拟合的一种方法。即 $ \\left\\|w\\right\\|_2 \\leq r$\n",
    "* 通过keras_constraint=keras.constraints.max_norm 来设定\n",
    "* 如果与卷积层一起使用，请确保 axis 正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "55000/55000 [==============================] - 11s 193us/sample - loss: 0.4743 - accuracy: 0.8339 - val_loss: 0.3882 - val_accuracy: 0.8572\n",
      "Epoch 2/2\n",
      "55000/55000 [==============================] - 10s 190us/sample - loss: 0.3566 - accuracy: 0.8703 - val_loss: 0.3436 - val_accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
