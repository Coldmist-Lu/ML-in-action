{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 BN 的基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 13s 239us/sample - loss: 0.8560 - accuracy: 0.7131 - val_loss: 0.5546 - val_accuracy: 0.8110\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.5790 - accuracy: 0.7987 - val_loss: 0.4782 - val_accuracy: 0.8380\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.5201 - accuracy: 0.8184 - val_loss: 0.4416 - val_accuracy: 0.8492\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.4836 - accuracy: 0.8305 - val_loss: 0.4198 - val_accuracy: 0.8570\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 12s 213us/sample - loss: 0.4577 - accuracy: 0.8388 - val_loss: 0.4036 - val_accuracy: 0.8628\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 12s 217us/sample - loss: 0.4397 - accuracy: 0.8441 - val_loss: 0.3934 - val_accuracy: 0.8682\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 12s 217us/sample - loss: 0.4246 - accuracy: 0.8505 - val_loss: 0.3842 - val_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.4110 - accuracy: 0.8547 - val_loss: 0.3764 - val_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.4025 - accuracy: 0.8590 - val_loss: 0.3712 - val_accuracy: 0.8748\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.3913 - accuracy: 0.8618 - val_loss: 0.3648 - val_accuracy: 0.8764\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 在激活函数前使用 BN\n",
    "\n",
    "* 需要将激活函数与网络层定义分开，且设置use_bias=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 13s 234us/sample - loss: 1.0145 - accuracy: 0.6822 - val_loss: 0.6564 - val_accuracy: 0.7954\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.6656 - accuracy: 0.7852 - val_loss: 0.5467 - val_accuracy: 0.8232\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 12s 212us/sample - loss: 0.5818 - accuracy: 0.8063 - val_loss: 0.4940 - val_accuracy: 0.8386\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 12s 211us/sample - loss: 0.5388 - accuracy: 0.8171 - val_loss: 0.4632 - val_accuracy: 0.8460\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 11s 209us/sample - loss: 0.5095 - accuracy: 0.8261 - val_loss: 0.4419 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 12s 213us/sample - loss: 0.4875 - accuracy: 0.8317 - val_loss: 0.4266 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 12s 210us/sample - loss: 0.4663 - accuracy: 0.8382 - val_loss: 0.4134 - val_accuracy: 0.8582\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 11s 207us/sample - loss: 0.4527 - accuracy: 0.8426 - val_loss: 0.4033 - val_accuracy: 0.8596\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 12s 211us/sample - loss: 0.4423 - accuracy: 0.8463 - val_loss: 0.3941 - val_accuracy: 0.8612\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 12s 214us/sample - loss: 0.4284 - accuracy: 0.8506 - val_loss: 0.3863 - val_accuracy: 0.8644\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 梯度裁剪\n",
    "\n",
    "* 裁剪梯度的思想是控制梯度不要超过某个阈值。这一技术在 RNN 中常用，因为 RNN 不能设置 BN 层。\n",
    "* 由 optimizer 的 clipvalue 和 clipnorm 参数控制。需要注意如果修改 clipvalue 可能会改变梯度的方向。所以最好用 clipnorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 迁移学习 - 预训练层重用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 在小批量样本上训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在大量样本上进行预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 8s 189us/sample - loss: 0.5909 - accuracy: 0.8100 - val_loss: 0.3772 - val_accuracy: 0.8707\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 8s 171us/sample - loss: 0.3520 - accuracy: 0.8793 - val_loss: 0.3400 - val_accuracy: 0.8772\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 8s 172us/sample - loss: 0.3167 - accuracy: 0.8890 - val_loss: 0.3018 - val_accuracy: 0.8954\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 7s 170us/sample - loss: 0.2969 - accuracy: 0.8969 - val_loss: 0.2859 - val_accuracy: 0.9028\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 7s 168us/sample - loss: 0.2824 - accuracy: 0.9030 - val_loss: 0.2808 - val_accuracy: 0.9066\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 8s 172us/sample - loss: 0.2721 - accuracy: 0.9073 - val_loss: 0.2684 - val_accuracy: 0.9103\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 8s 172us/sample - loss: 0.2641 - accuracy: 0.9097 - val_loss: 0.2683 - val_accuracy: 0.9106\n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 7s 168us/sample - loss: 0.2570 - accuracy: 0.9116 - val_loss: 0.2739 - val_accuracy: 0.9046\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 7s 169us/sample - loss: 0.2514 - accuracy: 0.9142 - val_loss: 0.2589 - val_accuracy: 0.9121\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 7s 168us/sample - loss: 0.2456 - accuracy: 0.9163 - val_loss: 0.2598 - val_accuracy: 0.9108\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 8s 174us/sample - loss: 0.2416 - accuracy: 0.9179 - val_loss: 0.2485 - val_accuracy: 0.9160\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 8s 173us/sample - loss: 0.2378 - accuracy: 0.9187 - val_loss: 0.2450 - val_accuracy: 0.9158\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 8s 172us/sample - loss: 0.2342 - accuracy: 0.9205 - val_loss: 0.2453 - val_accuracy: 0.9168\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 8s 171us/sample - loss: 0.2307 - accuracy: 0.9207 - val_loss: 0.2440 - val_accuracy: 0.9163\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 7s 167us/sample - loss: 0.2278 - accuracy: 0.9218 - val_loss: 0.2394 - val_accuracy: 0.9180\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 7s 166us/sample - loss: 0.2249 - accuracy: 0.9225 - val_loss: 0.2392 - val_accuracy: 0.9180\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 8s 171us/sample - loss: 0.2218 - accuracy: 0.9237 - val_loss: 0.2388 - val_accuracy: 0.9188\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 7s 170us/sample - loss: 0.2199 - accuracy: 0.9241 - val_loss: 0.2398 - val_accuracy: 0.9170\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 7s 169us/sample - loss: 0.2177 - accuracy: 0.9252 - val_loss: 0.2331 - val_accuracy: 0.9208\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 8s 172us/sample - loss: 0.2154 - accuracy: 0.9253 - val_loss: 0.2327 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"./models/Keras6-modelA.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接在 B 样本上训练试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 1s 5ms/sample - loss: 0.9545 - accuracy: 0.4600 - val_loss: 0.6655 - val_accuracy: 0.5385\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 821us/sample - loss: 0.5899 - accuracy: 0.6900 - val_loss: 0.4785 - val_accuracy: 0.8519\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 798us/sample - loss: 0.4512 - accuracy: 0.8800 - val_loss: 0.4098 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 825us/sample - loss: 0.3871 - accuracy: 0.9100 - val_loss: 0.3666 - val_accuracy: 0.9128\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 788us/sample - loss: 0.3438 - accuracy: 0.9250 - val_loss: 0.3315 - val_accuracy: 0.9300\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 796us/sample - loss: 0.3095 - accuracy: 0.9300 - val_loss: 0.3034 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 795us/sample - loss: 0.2810 - accuracy: 0.9400 - val_loss: 0.2808 - val_accuracy: 0.9432\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 782us/sample - loss: 0.2580 - accuracy: 0.9500 - val_loss: 0.2618 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 0s 803us/sample - loss: 0.2372 - accuracy: 0.9600 - val_loss: 0.2447 - val_accuracy: 0.9513\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 833us/sample - loss: 0.2196 - accuracy: 0.9650 - val_loss: 0.2316 - val_accuracy: 0.9513\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 817us/sample - loss: 0.2048 - accuracy: 0.9650 - val_loss: 0.2182 - val_accuracy: 0.9533\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 821us/sample - loss: 0.1915 - accuracy: 0.9650 - val_loss: 0.2071 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 831us/sample - loss: 0.1791 - accuracy: 0.9650 - val_loss: 0.1959 - val_accuracy: 0.9594\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 821us/sample - loss: 0.1681 - accuracy: 0.9700 - val_loss: 0.1864 - val_accuracy: 0.9604\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 803us/sample - loss: 0.1579 - accuracy: 0.9850 - val_loss: 0.1778 - val_accuracy: 0.9625\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 827us/sample - loss: 0.1491 - accuracy: 0.9850 - val_loss: 0.1695 - val_accuracy: 0.9675\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 819us/sample - loss: 0.1411 - accuracy: 0.9900 - val_loss: 0.1626 - val_accuracy: 0.9686\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 832us/sample - loss: 0.1339 - accuracy: 0.9900 - val_loss: 0.1570 - val_accuracy: 0.9686\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 800us/sample - loss: 0.1277 - accuracy: 0.9900 - val_loss: 0.1522 - val_accuracy: 0.9696\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 833us/sample - loss: 0.1217 - accuracy: 0.9900 - val_loss: 0.1471 - val_accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 B 样本集上微调 A 的预训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"./models/Keras6-modelA.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # 用 A 的网络层实现参数共享\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A) # 为了不影响 A 的参数，对 A 进行克隆\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]: # 前几个轮次冻结重用层\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.2787 - accuracy: 0.9250 - val_loss: 0.2925 - val_accuracy: 0.9229\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 698us/sample - loss: 0.2678 - accuracy: 0.9300 - val_loss: 0.2823 - val_accuracy: 0.9290\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 696us/sample - loss: 0.2578 - accuracy: 0.9350 - val_loss: 0.2728 - val_accuracy: 0.9310\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 709us/sample - loss: 0.2486 - accuracy: 0.9400 - val_loss: 0.2642 - val_accuracy: 0.9351\n",
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 1s 5ms/sample - loss: 0.2224 - accuracy: 0.9450 - val_loss: 0.2115 - val_accuracy: 0.9604\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 0s 773us/sample - loss: 0.1754 - accuracy: 0.9550 - val_loss: 0.1771 - val_accuracy: 0.9726\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 0s 824us/sample - loss: 0.1448 - accuracy: 0.9750 - val_loss: 0.1537 - val_accuracy: 0.9787\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 0s 855us/sample - loss: 0.1238 - accuracy: 0.9800 - val_loss: 0.1367 - val_accuracy: 0.9817\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 0s 811us/sample - loss: 0.1082 - accuracy: 0.9800 - val_loss: 0.1230 - val_accuracy: 0.9848\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 0s 807us/sample - loss: 0.0952 - accuracy: 0.9950 - val_loss: 0.1127 - val_accuracy: 0.9858\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 0s 834us/sample - loss: 0.0856 - accuracy: 0.9950 - val_loss: 0.1037 - val_accuracy: 0.9878\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 0s 813us/sample - loss: 0.0769 - accuracy: 0.9950 - val_loss: 0.0966 - val_accuracy: 0.9878\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 0s 818us/sample - loss: 0.0703 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9858\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 0s 814us/sample - loss: 0.0647 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9868\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 0s 788us/sample - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9868\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 0s 801us/sample - loss: 0.0558 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9878\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 0s 772us/sample - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9878\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 0s 796us/sample - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9878\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 0s 797us/sample - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9878\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 0s 803us/sample - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]: # 解冻\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),  # 可以考虑降低学习率\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14446662282943726, 0.9695]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.056635786026716234, 0.994]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B, verbose=0) # 准确率有效提高"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
